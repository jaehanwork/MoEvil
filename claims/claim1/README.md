### Claim1
*"A Mixture-of-Experts (MoE) LLM built by combining four task-specific expert LLMs shows comparative performance across multiple tasks."*

### Expected Resulst

<!-- | Header 1 | Header 2 | Header 3 |
|---|---|---|
| Data 1A | Data 2A | Data 3A |
| Data 1B | Data 2B | Data 3B | -->
